# Hi, I'm Riteng (Gavin) Zhang! üëã

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/0d4f23b0dfd2cf87812c37b1956610b011ae6418/IMG_5767%20(1).jpeg?raw=true" width="170" height="300" alt="Image Alt Text">

## About Me üåû
I am an undergraduate student at Boston College, passionate about researching deep neural network interpretability and building interpretable AI systems. I am majoring in Mathematics and Computer Science, complemented by a minor in Philosophy.

- üî≠ Currently working on my thesis and independent research project (advised by [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html) and [Professor Sergio Alvarez](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/sergio-alvarez.html)):
    - [**Dichotomy in Interpretability Approaches for Natural Language Processing**](https://github.com/ritengzhang/Interpretability-of-neural-language-model-A-survey)
    - [**Branch Specialization Analysis**](https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants)

- üîà Research Assistant for [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html) on topics including ASR for under-resourced language, Language Models Evaluation for Neuroatypical Language, etc.

- üåª Co-founder and machine learning engineer of [Blossoms ai](https://blossoms.ai/about).

- üì´ Contact Information: **zhangcoj@bc.edu**

- üìÑ Here's my [cv](https://drive.google.com/file/d/1ZpEfd9aEu7IrSBe2-Gp2I63rWzuLzGCW/view?usp=drive_link).

- ‚ö° Fun fact: I love history and philosophy, and regularly write about those on [social media](https://mp.weixin.qq.com/s?__biz=MzUxMzA5NTYwOA==&mid=2247483679&idx=1&sn=ce7cbf5a52b5e0b824578bdd6b1d764b&chksm=f95b23c8ce2caadeeb78f56216e3dcd88924e9793e035447cca656f0c9d9f2c163dd4e93e39d#rd) (modern Chinese history, Epistemology, and Existentialism in particular).

## Research Interest ‚úèÔ∏è
- Interpretability of black box model and its applications (trustworthy, robust, fair AI)
- Neural Architecture Search
- Interpretability in the Context of Large Language Models: Interpretation and Prediction of LLM Capabilities and Emergent Abilities

## Branch Specialization Analysis Project üå≥
The Branch Specialization Analysis Project is a project of my own that will have several stages. Currently, it's in the very early stages with a focus on providing baseline and evaluation metrics for branch specialization consistency.

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/e4a2a3af80d99f621597987b2ea102a648469d71/branch.png?raw=true" width="320" height="320" alt="Image Alt Text">

Branch specialization in the context of neural networks refers to the phenomenon where different branches, layers, or segments within a neural network develop or are designed to handle specific types of tasks or process specific kinds of information. In deep learning models, which often consist of complex and layered structures, not all parts of the network contribute equally or in the same way to the final output. Instead, certain branches might become more attuned to particular features or aspects of the data.

Understanding and analyzing branch specialization is crucial for several reasons:
- It aids in creating more interpretable models, as it becomes clearer what roles different parts of the network are playing.
- It can lead to more efficient network designs, where unnecessary or redundant branches can be identified and pruned without loss of overall functionality.
- It provides insights that can be utilized in neural architecture search (NAS) to design optimized and task-specific models.

[**Analyzing Variations in Layer-wise Feature Attributions of Inception Model**](https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants) (advised by [Professor Sergio Alvarez](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/sergio-alvarez.html))

[<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/main/inceptionpic.jpg?raw=true" width="700" height="500" alt="Image Alt Text">](https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants)

This paper investigates the variability in layer feature attribution across different branches in the One-Layer Inception neural network architecture. Despite using consistent datasets, model architectures, and hyperparameters, training with different initial parameters leads to differences in neuron roles and contributions. Our focus is on how the choice of hyperparameters affects the variation in the contribution of each branch in the One-Layer Inception model. Using various datasets, we conducted 15 training sessions with the  One-Layer Inception model for each hyperparameter combination. The study reveals that factors like larger batch sizes and lower learning rates lead to a reduced variance in feature attribution across different training sessions. In addition, variation in layer feature attribution is lower for models trained on more complex datasets.

**Future stages**
- Employing methods like Probing to assess branch specialization consistency across various neural models more directly. 
- Extending evaluation beyond one-layer models to include more complex branched architectures, analyzing deeper layers for branch specialization consistency principles.
- Exploring additional aspects of branch specialization, not limited to consistency evaluation.

## Other Research üìñ
- [**Dichotomy in Interpretability Approaches for Natural Language Processing**](https://github.com/ritengzhang/Interpretability-of-neural-language-model-A-survey) (advised by [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html))

[<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/main/%E4%B8%8B%E8%BD%BD.png?raw=true" width="500" height="400" alt="Image Alt Text">](https://github.com/ritengzhang/Interpretability-of-neural-language-model-A-survey)

Inspired by Table I in the paper titled "[1]Post-hoc Interpretability for Neural NLP: A Survey," I have systematically categorized interpretability methods into 17 spectrums, such as Global vs. Local and Post-hoc vs. Inherent interpretability. This paper provides a more structured examination of current interpretability methods and their applications. Additionally, it includes discussions on the connection of interpretability to Large Language Models (LLMs), such as works on interpreting and predicting the emergent abilities or capacities of LLMs.

- [**Evaluation of LLM Zero to Few-Shot Ability when Expecting Formatted Output**](https://github.com/ritengzhang/Evaluation-of-LLM-Zero-to-Few-Shot-Ability-when-Expecting-Formatted-Output) (advised by [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html))

[<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/main/gptformatedpic.jpg?raw=true" width="700" height="400" alt="Image Alt Text">](https://github.com/ritengzhang/Evaluation-of-LLM-Zero-to-Few-Shot-Ability-when-Expecting-Formatted-Output)

These experiments are designed to systematically evaluate the performance of GPT models under various conditions, focusing on their ability to generate formatted output and provide accurate answers. The collected metrics aid in assessing the models' capabilities and limitations in handling diverse scenarios and formatting requirements. More details can be found in the repository. The ideas presented in papers like "[2]Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", which question the contribution of intrusive labels and instructive instructions, are also tested in these experiments. However, the tested GPT family model performs too well on the chosen dataset to observe a significant difference. The retrievable answers are usually the correct choices from the output space.

- **Large ASR Model Evaluation for Under-resourced Language** (PI: [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html))

- **Language Models Evaluation for Neuroatypical Language** (PI: [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html))

## Publications üìÑ
- **Leveraging LLMs and MLPs in Designing a Computer Science Placement Test System** (In Proceedings of CSCI 2023, Coauthored with Yi LI and Angela Qu, advised by [Professor Maira Samary](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/maira-samary.html))

[<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/main/cs%20replacement%20pic.jpg?raw=true" width="700" height="410" alt="Image Alt Text">]()

Different types of models, including LLMs, are utilized to create an automated process for conducting a Computer Science (CS) placement test in a step-by-step manner. The framework's limitations and potential are discussed.

## Startup - [Blossoms ai](https://blossoms.ai/about) ü§ñ‚ûïüéì
[<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/main/blossompic.jpg?raw=true" width="700" height="450" alt="Image Alt Text">](https://blossoms.ai/about)

At BlossomsAI, we are on a mission to transform education by leveraging the power of artificial intelligence. We firmly believe that a personalized, individualized, and customized approach to education can unlock the full potential of every child. Our goal is to equip teachers with the tools and resources necessary to save time, increase efficiency, and focus on nurturing each student's unique abilities and interests.

As co-founder and machine learning engineer in the company, I am responsible for everything related to AI or ML in BlossomsAI, including necessary research and back-end coding using Pytorch customized models, hugging face models, LLMs APIs for many different usages expected in our product.

## Reference
[1] Madsen, Andreas, Siva Reddy, and Sarath Chandar. "[Post-hoc Interpretability for Neural NLP: A Survey](https://arxiv.org/abs/2108.04840)." ACM Computing Surveys 55, no. 8 (2022): 1-42.

[2] Min, Sewon, et al. "[Rethinking the role of demonstrations: What makes in-context learning work?](https://arxiv.org/abs/2202.12837)." arXiv preprint arXiv:2202.12837 (2022).

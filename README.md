# Riteng (Gavin) Zhang

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/11348b56d16f33c458205b8bb4a59f97f87b4524/1.jpeg?raw=true" align="left" width="170" height="200" alt="Riteng (Gavin) Zhang"> I am an undergraduate student at Boston College, passionate about researching deep neural network interpretability and building interpretable AI systems. I am majoring in Mathematics and Computer Science, complemented by a minor in Philosophy.

<br clear="all"/>

## About Me

- üî≠ Currently working on my thesis and independent research project (advised by [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html) and [Professor Sergio Alvarez](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/sergio-alvarez.html)):
    - [**Interpretability Formalizing and Automating Framework**](https://github.com/ritengzhang/Interpretability-Formalizing-and-Auto-Explaining-Framework)
    - [**Branch Specialization Analysis**](https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants)

- üîà Research Assistant for [Professor Emily Prud'hommeaux](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/emily-prudhommeaux.html) on topics including ASR for under-resourced language, Language Models Evaluation for Neuroatypical Language, etc.
  
- üí¨ Teaching assistant for **CSCI3399 Vision and Learning, CSCI3345 Machine Learning** in Boston College

- üåª Co-founder and machine learning engineer of [Blossoms ai](https://blossoms.ai).

- üì´ Contact Information: **zhangcoj@bc.edu**

- üìÑ Here's my [cv](https://drive.google.com/file/d/1VewTSBcugWXg4MtbWEE_Xbm5PfEAokhD/view?usp=drive_link).

- ‚ö° Fun fact: I love history and philosophy (Modern Chinese, Ancient Roman history, Epistemology, and Existentialism in particular).

## Research Interest ‚úèÔ∏è

- Interpretability of black box model and its applications (trustworthy, robust, fair AI)
- Deep learning model Modularity
- Neural Architecture Search
- Interpretability in the Context of Large Language Models: Interpretation and Prediction of LLM Capabilities and Emergent Abilities

## Branch Specialization Analysis Project üå≥

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/e4a2a3af80d99f621597987b2ea102a648469d71/branch.png?raw=true" align="left" width="300" height="300" alt="Branch Specialization Analysis">

The Branch Specialization[3] Analysis Project is a project of my own that will have several stages. Currently, it's in the very early stages with a focus on providing baseline and evaluation metrics for branch specialization consistency and exploring the potential of branch specialization in combining the functional and architectural modularity of deep learning models. Understanding and analyzing branch specialization is crucial for several reasons:

&nbsp; - It aids in creating more interpretable models, as it becomes clearer what roles different parts of the network are playing.

&nbsp; -  It can lead to more efficient network designs, where unnecessary or redundant branches can be identified and pruned without loss of overall functionality.

&nbsp; - It provides insights that can be utilized in neural architecture search (NAS) to design optimized and task-specific models.

<br clear="all"/>

**Papers under this project:**

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/7bbe45562d857a1c6b3b73c32e9fb1cbcacf4205/pic/branch%20attribution.jpg?raw=true" align="left" width="300" height="300" alt="Analyzing Variations in Layer-wise Feature Attributions">

[**Analyzing Variations in Branch Attribution in Non-monolithic Models**](https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants) (advised by [Professor Sergio Alvarez](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/sergio-alvarez.html))

This paper investigates the variability in layer feature attribution across different branches in various branched neural networks (monolithic design vs. inception-like). Despite using consistent datasets, model architectures, and hyperparameters, training with different initial parameters leads to differences in neuron roles and contributions. Our focus is on determining whether the monolithic design of branched models will have higher variation in its branch attribution than that of inception-like models or non-monolithic branched neural networks.

<br clear="all"/>

## Other Research üìñ

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/95fd2bb4a673e4454ec96db79923a9f203338d7e/method%20encoding%20pic.jpg?raw=true" align="left" width="300" height="300" alt="Interpretability Formalizing and Auto-Explaining Framework">

[**Interpretability Formalizing and Automating Framework**](https://github.com/ritengzhang/Interpretability-of-neural-language-model-A-survey) 

Inspired by "Post-hoc Interpretability for Neural NLP: A Survey," I aimed to encode interpretability methods into a structured, formal framework using Python classes, this new project seeks to establish a unified representation that automatically captures the essence (functionality and application) of deep learning interpretability methods across various dimensions. It categorizes interpretability methods based on characteristics such as global vs. local, similar to what previous surveys have done, and evaluates their complexity, fidelity, etc. The project also hopes to generate new methods through innovative approaches, such as generative sequence models (like tree RNNs).

<br clear="all"/>

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/7bbe45562d857a1c6b3b73c32e9fb1cbcacf4205/pic/llm%20few%20shot.jpg?raw=true" align="left" width="300" height="300" alt="Evaluation of LLM Zero to Few-Shot Ability">

[**Evaluation of LLM Zero to Few-Shot Ability when Expecting Formatted Output**](https://github.com/ritengzhang/Evaluation-of-LLM-Zero-to-Few-Shot-Ability-when-Expecting-Formatted-Output) 

These experiments are designed to systematically evaluate the performance of GPT models under various conditions, focusing on their ability to generate formatted output and provide accurate answers. The collected metrics aid in assessing the models' capabilities and limitations in handling diverse scenarios and formatting requirements.

<br clear="all"/>

## Publications üìÑ

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/7bbe45562d857a1c6b3b73c32e9fb1cbcacf4205/pic/cs%20palcement%20llms.jpg?raw=true" align="left" width="300" height="300" alt="CS Placement Test System">

**Leveraging LLMs and MLPs in Designing a Computer Science Placement Test System** (In Proceedings of CSCI 2023, Coauthored with Yi LI and Angela Qu, advised by [Professor Maira Samary](https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/maira-samary.html)) 

Different types of models, including LLMs, are utilized to create an automated process for conducting a Computer Science (CS) placement test in a step-by-step manner. The framework's limitations and potential are discussed.

<br clear="all"/>

## Startup - [Blossoms ai](https://blossoms.ai) ü§ñ‚ûïüéì

<img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/7bbe45562d857a1c6b3b73c32e9fb1cbcacf4205/pic/K-12%20Education.jpg?raw=true" align="left" width="300" height="300" alt="Blossoms ai">

As CAIO and co-founder of BlossomsAI, I am deeply committed to transforming education through the power of artificial intelligence. At BlossomsAI, we believe that a personalized, individualized, and customized approach is key to unlocking every student's full potential. Our goal is to provide teachers with the tools and resources they need to save time, increase efficiency, and focus on nurturing the unique abilities and interests of each student.

<br clear="all"/>


## Awards & Achievements üèÜ

- **John McCarthy, S.J., Award for Natural Science** (Boston College, May 2024): Awarded annually to one student for the best senior thesis (Interpretability Formalizing and Automating Framework) in Natural Science, judged by dean's reviewers and administered by the Morrissey College of Arts and Sciences Dean‚Äôs Office.

- **Neuhauser Award** (Boston College, May 2024): Given annually to 1-2 students in the Computer Science department at Boston College who achieve the most recognition from all CS faculty.

- **Scholar of the College** (Boston College, May 2024): Awarded to exceptional students who have excelled academically and completed substantial, high-quality independent work under faculty supervision during their senior year.

## Travel ‚úàÔ∏è
ACL 2023

NeurIPS 2023

## Talk
Understanding LSTM Networks, Boston College Experimental Math & ML lab, Nov 2023

## Reference
[1] Madsen, Andreas, Siva Reddy, and Sarath Chandar. "[Post-hoc Interpretability for Neural NLP: A Survey](https://arxiv.org/abs/2108.04840)." ACM Computing Surveys 55, no. 8 (2022): 1-42.

[2] Min, Sewon, et al. "[Rethinking the role of demonstrations: What makes in-context learning work?](https://arxiv.org/abs/2202.12837)." arXiv preprint arXiv:2202.12837 (2022).

[3] Voss, Chelsea, et al. "[Branch Specialization](https://distill.pub/2020/circuits/branch-specialization/)." Distill (2021).

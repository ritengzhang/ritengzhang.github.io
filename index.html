<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Riteng (Gavin) Zhang</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
        <div class="container">
        <h1>Riteng (Gavin) Zhang</h1>
        <img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/11348b56d16f33c458205b8bb4a59f97f87b4524/1.jpeg?raw=true" align="left" width="200" height="230" alt="Riteng (Gavin) Zhang">
        <p>I graduated from Boston College with majors in Mathematics and Computer Science, and a minor in Philosophy. Currently, I serve as the Chief AI Officer (CAIO) at Blossoms AI, an educational technology startup I co-founded. My academic passion lies in ML/AI interpretability.</p>
        
        <p>All my specific research interests listed below align with my main guiding causes: first, to design AI systems that are inherently interpretable and modular, with planned, explainable behavior and capabilities. Second, I wish to develop Microscope AI Systems to assist us in other academic fields by training models and exploring the knowledge they have learned, which we don't yet know, using interpretability.</p>
        
        <p>Beyond AI-related topics, my academic interests span history, epistemology, philosophy, theology, education, cosmology, and neuroscience. The dynamic interplay of these fields inspires helpful insights and whimsical dreams in both my industrial and academic life.</p>
        
        <p>I am pursuing a PhD in the AI/ML field during the 2024/2025 application term. Please feel free to reach out.</p>
        
        <br clear="all"/>
        </div>





        <h2>üì´ Contact Information</h2>
        <p><strong>ritengzhang77@gmail.com</strong></p>

        <h2>üìÑ <a href="https://drive.google.com/file/d/1kcpt9PucA9Mme6l5vXPsfi8MnWqTT7aO/view?usp=drive_link">My CV</a></h2>


        
        <h2>üîç Research Interests</h2>
        <ul>
            <li>Mechanistic Interpretability</li>
            <li>Deep Learning Model Modularity</li>
            <li>Interpretability in LLM or advanced Models (Emergent Ability/Behavior)</li>
            <li>Formalization and Automation of Interpretable ML Systems</li>
            <li>Neural Architecture Search</li>
            <li>Transparent, Trustworthy, Robust AI; ML Safety</li>
        </ul>

        <h2>üèÜ Awards & Achievements</h2>
        <ul>
            <li><strong>John McCarthy, S.J., Award for Natural Science</strong> (Boston College, May 2024): Awarded annually to one student for the best senior thesis (mine is Interpretability Formalizing and Automating Framework) in Natural Science, judged by dean's reviewers and administered by the Morrissey College of Arts and Sciences Dean‚Äôs Office.</li>
            <li><strong>Neuhauser Award</strong> (Boston College, May 2024): Given annually to 1-2 students in the Computer Science department at Boston College who achieve the most recognition from all CS faculty.</li>
            <li><strong>Scholar of the College</strong> (Boston College, May 2024): Awarded to exceptional students who have excelled academically and completed substantial, high-quality independent work under faculty supervision during their senior year.</li>
        </ul>

        
        <h2>ü§ñ‚ûïüéì Startup - <a href="https://blossoms.ai">Blossoms AI</a></h2>
        <img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/1a01755368f74b77d7cccdb9f7ed3c48d9a6199f/image.png?raw=true" align="left" width="400" height="150" alt="Blossoms AI">
        <p>As CAIO and co-founder of Blossoms AI, I am dedicated to transforming education through artificial intelligence. Our mission is to address the shortcomings of the current education system, especially in the age of AI, and to build a new one using AI and other innovative tools. Our application is set to be deployed in several high schools and potentially colleges next semester, helping teachers save time, increase efficiency, and focus on nurturing each student's unique abilities and interests.</p>
        <p>My roles at Blossoms AI include:</p>
        <ul>
            <li>Managing an AI engineering team of 6-7 people responsible for designing, developing, testing, and monitoring AI products in education technology.</li>
            <li>Developed and monitoring tools aimed at reshaping higher education norms, such as Auto-Oral Defense, Mastery Level System, ThinkAid, and Adaptive Extra-Credit Assignment System.</li>
            <li>Created a tool that automatically performs data analysis and generates insightful reports or engages in user chats. This tool can be utilized by students, instructors, school administrators, and school districts with different levels of authorization and is applicable to databases in any field.</li>
        </ul>
        <br clear="all"/>

        <h2>üå≥ Branch Specialization Analysis Project</h2>
        <img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/e4a2a3af80d99f621597987b2ea102a648469d71/branch.png?raw=true" align="left" width="300" height="300" alt="Branch Specialization Analysis">
        <p>The Branch Specialization Analysis Project is a project of my own that will have several stages. Currently, it's in the very early stages with a focus on providing baseline and evaluation metrics for branch specialization consistency and exploring the potential of branch specialization in combining the functional and architectural modularity of deep learning models. Understanding and analyzing branch specialization is crucial for several reasons:</p>
        <ul>
            <li>It aids in creating more interpretable models, as it becomes clearer what roles different parts of the network are playing.</li>
            <li>It can lead to more efficient network designs, where unnecessary or redundant branches can be identified and pruned without loss of overall functionality.</li>
            <li>It provides insights that can be utilized in neural architecture search (NAS) to design optimized and task-specific models.</li>
        </ul>
        <br clear="all"/>

        <h3>Papers under this project:</h3>
        <img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/7bbe45562d857a1c6b3b73c32e9fb1cbcacf4205/pic/branch%20attribution.jpg?raw=true" align="left" width="300" height="300" alt="Analyzing Variations in Layer-wise Feature Attributions">
        <p><a href="https://github.com/ritengzhang/Interpretability-of-Inception-and-its-variants"><strong>Analyzing Variations in Branch Attribution in Non-monolithic Models</strong></a> (advised by <a href="https://www.bc.edu/bc-web/schools/mcas/departments/computer-science/people/faculty-directory/sergio-alvarez.html">Professor Sergio Alvarez</a>)</p>
        <p>This paper investigates the variability in layer feature attribution across different branches in various branched neural networks (monolithic design vs. inception-like). Despite using consistent datasets, model architectures, and hyperparameters, training with different initial parameters leads to differences in neuron roles and contributions. Our focus is on determining whether the monolithic design of branched models will have higher variation in its branch attribution than that of inception-like models or non-monolithic branched neural networks.</p>
        <br clear="all"/>

        <h2>üìñ Other Notable Research</h2>
        <img src="https://github.com/ritengzhang/ritengzhang.github.io/blob/95fd2bb4a673e4454ec96db79923a9f203338d7e/method%20encoding%20pic.jpg?raw=true" align="left" width="300" height="300" alt="Interpretability Formalizing and Auto-Explaining Framework">
        <p><a href="https://github.com/ritengzhang/Interpretability-of-neural-language-model-A-survey"><strong>Interpretability Formalizing and Automating Framework</strong></a></p>
        <p>Inspired by "Post-hoc Interpretability for Neural NLP: A Survey," I aimed to encode interpretability methods into a structured, formal framework using Python classes. This new project seeks to establish a unified representation that automatically captures the essence (functionality and application) of deep learning interpretability methods across various dimensions. It categorizes interpretability methods based on characteristics such as global vs. local, similar to what previous surveys have done, and evaluates their complexity, fidelity, etc. The project also hopes to generate new methods through innovative approaches, such as generative sequence models (like tree RNNs).</p>
        <br clear="all"/>

        <h2>‚úàÔ∏è Travel</h2>
        <ul>
            <li>ACL 2023</li>
            <li>NeurIPS 2023</li>
        </ul>

        <h2>üì¢ Talk</h2>
        <p><strong>Understanding LSTM Networks</strong>, Boston College Experimental Math & ML lab, Nov 2023</p>

        <h2>üìö References</h2>
        <ul>
            <li>Madsen, Andreas, Siva Reddy, and Sarath Chandar. "<a href="https://arxiv.org/abs/2108.04840">Post-hoc Interpretability for Neural NLP: A Survey</a>." ACM Computing Surveys 55, no. 8 (2022): 1-42.</li>
            <li>Min, Sewon, et al. "<a href="https://arxiv.org/abs/2202.12837">Rethinking the role of demonstrations: What makes in-context learning work?</a>" arXiv preprint arXiv:2202.12837 (2022).</li>
            <li>Voss, Chelsea, et al. "<a href="https://distill.pub/2020/circuits/branch-specialization/">Branch Specialization</a>." Distill (2021).</li>
        </ul>
    </div>
</body>
</html>
